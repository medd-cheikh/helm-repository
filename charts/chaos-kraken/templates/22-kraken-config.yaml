{{- if .Values.kraken.enabled -}}
{{- $root := . -}}
{{- $appName := include "startx.appName" . -}}
{{- $namespace := .Values.project.project.name -}}
{{- if .Values.kraken.aws -}}{{- if .Values.kraken.aws.enabled -}}
{{- with .Values.kraken.aws }}
---
kind: Secret
apiVersion: v1
type: Opaque
metadata:
  name: kraken-aws-creds
  namespace: "{{- $namespace -}}"
  labels:
    app.kubernetes.io/name: "kraken-aws-creds"
    {{- include "chaos-kraken.labels" $root | nindent 4 }}
  annotations:
    {{- include "chaos-kraken.annotations" $root | nindent 4 }}
stringData:
  AWS_ACCESS_KEY_ID: {{ .credentials.key_id | default "AWS_ACCESS_KEY_ID" | quote }}
  AWS_DEFAULT_REGION: {{ .credentials.region | default "AWS_DEFAULT_REGION" | quote }}
  AWS_SECRET_ACCESS_KEY: {{ .credentials.secret | default "AWS_SECRET_ACCESS_KEY" | quote }}
---
kind: Secret
apiVersion: v1
type: Opaque
metadata:
  name: kraken-vcloud-creds
  namespace: "{{- $namespace -}}"
  labels:
    app.kubernetes.io/name: "kraken-vcloud-creds"
    {{- include "chaos-kraken.labels" $root | nindent 4 }}
  annotations:
    {{- include "chaos-kraken.annotations" $root | nindent 4 }}
stringData:
  TENANT_URL: ''
  VAPP_NAME: ''
  REFRESH_TOKEN: ''
{{ end }}
{{- end -}}{{- end -}}
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kraken-scenarios-config
  namespace: "{{- $namespace -}}"
  labels:
    test: essai
    app.kubernetes.io/name: "kraken-scenarios-config"
    {{- include "chaos-kraken.labels" $root | nindent 4 }}
  annotations:
    {{- include "chaos-kraken.annotations" $root | nindent 4 }}
data:
  app_outage.yaml: |
    application_outage:                                  # Scenario to create an outage of an application by blocking traffic
      duration: 600                                      # Duration in seconds after which the routes will be accessible
      namespace: <namespace-with-application>            # Namespace to target - all application routes will go inaccessible if pod selector is empty
      pod_selector: {app: foo}                            # Pods to target
      block: [Ingress, Egress]                           # It can be Ingress or Egress or Ingress, Egress
  cluster_shut_down_scenario.yaml: |
    - id: run_python
      config:
        filename: scenarios/cluster_shut_down_scenario.py
  cluster_shut_down_scenario.py: |
    #!/usr/bin/env python3
    import subprocess
    import logging
    import time
    import http.client
    import json
    import xml.etree.ElementTree as ET
    import os

    tenant_url = os.environ.get('TENANT_URL')
    vapp_name = os.environ.get('VAPP_NAME')
    refresh_token = os.environ.get('REFRESH_TOKEN')

    def auth_vcloud(refreshToken):
        conn = http.client.HTTPSConnection(tenant_url)
        payload = 'grant_type=refresh_token&refresh_token='+refreshToken
        headers = {
        'Accept': 'application/json',
        'Content-Type': 'application/x-www-form-urlencoded'
        }
        conn.request("POST", "/oauth/tenant/C0000001-004/token", payload, headers)
        res = conn.getresponse()
        data = res.read()
        jsondata = json.loads(data.decode("utf-8"))
        return jsondata["access_token"]

    def vapp_name_to_vappId(access_token, vapp_name):
        conn = http.client.HTTPSConnection(tenant_url)
        payload = ''
        headers = {
        'Accept': '*/*;version=36.2',
        'Authorization': 'Bearer '+ access_token
        }
        conn.request("GET", "/api/vApps/query?pageSize=128", payload, headers)
        res = conn.getresponse()
        data = res.read()
        root = ET.fromstring(data)
        # for child in root:
        #   print(child.tag, child.attrib)
        for vapprecord in root.findall('{http://www.vmware.com/vcloud/v1.5}VAppRecord'):
            href = vapprecord.get('href')
            name = vapprecord.get('name')
            if name == vapp_name:
                vapp_id = href.removeprefix('https://console.monacocloud.app/api/vApp/')
                return vapp_id

    def cluster_reboot(access_token, vapp_id): # cluster_name : data or mgmt
        conn = http.client.HTTPSConnection(tenant_url)
        payload = ''
        headers = {
        'Accept': '*/*;version=36.2',
        'Authorization': 'Bearer '+ access_token
        }
        conn.request("POST", "/api/vApp/"+vapp_id+"/power/action/reboot", payload, headers)
        res = conn.getresponse()
        data = res.read()
        print(data.decode("utf-8"))

    def run(cmd):
        try:
            output = subprocess.Popen(
                cmd, shell=True, universal_newlines=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT
            )
            (out, err) = output.communicate()
        except Exception as e:
            logging.error("Failed to run %s, error: %s" % (cmd, e))
        return out

    cluster_reboot(auth_vcloud(refresh_token), vapp_name_to_vappId(auth_vcloud(refresh_token), vapp_name))
  customapp_pod.yaml: |
    - id: kill-pods
      config:
        namespace_pattern: ^acme-air$
        name_pattern: .*
    - id: wait-for-pods
      config:
        namespace_pattern: ^acme-air$
        name_pattern: .*
        count: 8
  post_action_etcd_container.py: |
    #!/usr/bin/env python3
    import subprocess
    import logging
    import time


    def run(cmd):
        try:
            output = subprocess.Popen(
                cmd, shell=True, universal_newlines=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT
            )
            (out, err) = output.communicate()
        except Exception as e:
            logging.error("Failed to run %s, error: %s" % (cmd, e))
        return out


    i = 0
    while i < 100:
        pods_running = run("oc get pods -n openshift-etcd -l app=etcd | grep -c '4/4'").rstrip()
        if pods_running == "3":
            break
        time.sleep(5)
        i += 1

    if pods_running == str(3):
        print("There were 3 pods running properly")
    else:
        print("ERROR there were " + str(pods_running) + " pods running instead of 3")
  post_action_etcd_example_py.py: |
    #!/usr/bin/env python3
    import subprocess
    import logging


    def run(cmd):
        try:
            output = subprocess.Popen(
                cmd, shell=True, universal_newlines=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT
            )
            (out, err) = output.communicate()
            logging.info("out " + str(out))
        except Exception as e:
            logging.error("Failed to run %s, error: %s" % (cmd, e))
        return out


    pods_running = run("oc get pods -n openshift-etcd | grep -c Running").rstrip()

    if pods_running == str(3):
        print("There were 3 pods running properly")
    else:
        print("ERROR there were " + str(pods_running) + " pods running instead of 3")
  post_action_shut_down.py: |
    #!/usr/bin/env python3
    import subprocess
    import logging
    import time
    import yaml


    def run(cmd):
        out = ""
        try:
            output = subprocess.Popen(
                cmd, shell=True, universal_newlines=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT
            )
            (out, err) = output.communicate()
        except Exception as e:
            logging.info("Failed to run %s, error: %s" % (cmd, e))
        return out


    # Get cluster operators and return yaml
    def get_cluster_operators():
        operators_status = run("kubectl get co -o yaml")
        status_yaml = yaml.safe_load(operators_status, Loader=yaml.FullLoader)
        return status_yaml


    # Monitor cluster operators
    def monitor_cluster_operator(cluster_operators):
        failed_operators = []
        for operator in cluster_operators["items"]:
            # loop through the conditions in the status section to find the dedgraded condition
            if "status" in operator.keys() and "conditions" in operator["status"].keys():
                for status_cond in operator["status"]["conditions"]:
                    # if the degraded status is not false, add it to the failed operators to return
                    if status_cond["type"] == "Degraded" and status_cond["status"] != "False":
                        failed_operators.append(operator["metadata"]["name"])
                        break
            else:
                logging.info("Can't find status of " + operator["metadata"]["name"])
                failed_operators.append(operator["metadata"]["name"])
        # return False if there are failed operators else return True
        return failed_operators


    wait_duration = 10
    timeout = 900
    counter = 0

    counter = 0
    co_yaml = get_cluster_operators()
    failed_operators = monitor_cluster_operator(co_yaml)
    while len(failed_operators) > 0:
        time.sleep(wait_duration)
        co_yaml = get_cluster_operators()
        failed_operators = monitor_cluster_operator(co_yaml)
        if counter >= timeout:
            print("Cluster operators are still degraded after " + str(timeout) + "seconds")
            print("Degraded operators " + str(failed_operators))
            exit(1)
        counter += wait_duration

    not_ready = run("oc get nodes --no-headers | grep 'NotReady' | wc -l").rstrip()
    while int(not_ready) > 0:
        time.sleep(wait_duration)
        not_ready = run("oc get nodes --no-headers | grep 'NotReady' | wc -l").rstrip()
        if counter >= timeout:
            print("Nodes are still not ready after " + str(timeout) + "seconds")
            exit(1)
        counter += wait_duration

    worker_nodes = run("oc get nodes --no-headers | grep worker | egrep -v NotReady | awk '{print $1}'").rstrip()
    print("Worker nodes list \n" + str(worker_nodes))
    master_nodes = run("oc get nodes --no-headers | grep master | egrep -v NotReady | awk '{print $1}'").rstrip()
    print("Master nodes list \n" + str(master_nodes))
    infra_nodes = run("oc get nodes --no-headers | grep infra | egrep -v NotReady | awk '{print $1}'").rstrip()
    print("Infra nodes list \n" + str(infra_nodes))
  startx_pvc_fruitapp1-preprod-postgresql.yaml: |
    pvc_scenario:
      pvc_name: fruitapp1-preprod-postgresql
      namespace: fruitapp1-preprod
      fill_percentage: 99
      duration: 30
  startx_pvc_fruitapp2-prod-postgresql.yaml: |
    pvc_scenario:
      pvc_name: fruitapp2-prod-postgresql
      namespace: fruitapp2-prod
      fill_percentage: 99
      duration: 30
  startx_node_app_cpu_chaos.yaml: |
    apiVersion: litmuschaos.io/v1alpha1
    kind: ChaosEngine
    metadata:
      name: nginx-chaos
      namespace: litmus
    spec:
      annotationCheck: 'false'
      engineState: 'active'
      chaosServiceAccount: litmus-sa
      monitoring: false
      jobCleanUpPolicy: 'delete'
      experiments:
        - name: node-cpu-hog
          spec:
            components:
              env:
                - name: TOTAL_CHAOS_DURATION
                  value: '60'
                - name: NODE_CPU_CORE
                  value: '1'
                - name: NODES_AFFECTED_PERC
                  value: '75'
                - name: NODE_LABEL
                  value: 'node-role.kubernetes.io/worker=""'
  startx_node_app_network_chaos.yaml: |
    network_chaos:
      duration: 30
      node_name: 
      label_selector: 'node-role.kubernetes.io/worker'
      instance_count: 1
      interfaces:
      - "br-ex"
      - "br-int"
      - "lo"
      - "ens192"
      execution: serial
      egress:
        latency: 50ms  
        loss: 0.05
        bandwidth: 100mbit
  startx_node_master_network_chaos.yaml: |
    network_chaos:
      duration: 30
      node_name: 
      label_selector: 'node-role.kubernetes.io/master'
      instance_count: 1
      interfaces:
      - "br-ex"
      - "br-int"
      - "lo"
      - "ens192"
      execution: serial
      egress:
        latency: 50ms  
        loss: 0.05
        bandwidth: 100mbit
  startx_node_infra_network_chaos.yaml: |
    network_chaos:
      duration: 30
      node_name: 
      label_selector: 'node-role.kubernetes.io/infra'
      instance_count: 1
      interfaces:
      - "br-ex"
      - "br-int"
      - "lo"
      - "ens192"
      execution: serial
      egress:
        latency: 50ms  
        loss: 0.05
        bandwidth: 100mbit
  startx_node_master.yaml: |
    - id: run_python
      config:
        filename: scenarios/startx_node_master.py
  startx_node_master.py: |
    #!/usr/bin/env python3
    import subprocess
    import logging
    import time
    import http.client
    import json
    import xml.etree.ElementTree as ET
    import os
    import random

    tenant_url = os.environ.get('TENANT_URL')
    refresh_token = os.environ.get('REFRESH_TOKEN')

    def auth_vcloud(refreshToken):
        conn = http.client.HTTPSConnection(tenant_url)
        payload = 'grant_type=refresh_token&refresh_token='+refreshToken
        headers = {
        'Accept': 'application/json',
        'Content-Type': 'application/x-www-form-urlencoded'
        }
        conn.request("POST", "/oauth/tenant/C0000001-004/token", payload, headers)
        res = conn.getresponse()
        data = res.read()
        jsondata = json.loads(data.decode("utf-8"))
        return jsondata["access_token"]

    def node_name_to_vmId(access_token, node_name):
        conn = http.client.HTTPSConnection(tenant_url)
        payload = ''
        headers = {
        'Accept': '*/*;version=36.2',
        'Authorization': 'Bearer '+ access_token
        }
        conn.request("GET", "/api/vms/query?pageSize=128", payload, headers)
        res = conn.getresponse()
        data = res.read()
        root = ET.fromstring(data)
        for vmrecord in root.findall('{http://www.vmware.com/vcloud/v1.5}VMRecord'):
            href = vmrecord.get('href')
            name = vmrecord.get('name')
            if name == node_name:
                vm_id = href.removeprefix('https://'+tenant_url+'/api/vApp/')
                return vm_id

    def node_reboot(access_token, vm_id):
        conn = http.client.HTTPSConnection(tenant_url)
        payload = ''
        headers = {
        'Accept': '*/*;version=36.2',
        'Authorization': 'Bearer '+ access_token
        }
        conn.request("POST", "/api/vApp/"+vm_id+"/power/action/reboot", payload, headers)
        res = conn.getresponse()
        data = res.read()
        print(data.decode("utf-8"))

    def run(cmd):
        try:
            output = subprocess.Popen(
                cmd, shell=True, universal_newlines=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT
            )
            (out, err) = output.communicate()
        except Exception as e:
            logging.error("Failed to run %s, error: %s" % (cmd, e))
        return out

    master_nodes = run("oc get nodes -l node-role.kubernetes.io/master='' -o custom-columns=NODE:.metadata.name --no-headers ").rstrip()
    masters = master_nodes.split('\n')
    random_master_index = random.randint(0,2)
    stop_kublet = run("oc debug node/" + masters[random_master_index] + " -- chroot /host systemctl stop kubelet").rstrip()
    node_reboot(auth_vcloud(refresh_token), node_name_to_vmId(auth_vcloud(refresh_token), masters[random_master_index]))
  startx_node_infra.yaml: |
    - id: run_python
      config:
        filename: scenarios/startx_node_infra.py
  startx_node_infra.py: |
    #!/usr/bin/env python3
    import subprocess
    import logging
    import time
    import http.client
    import json
    import xml.etree.ElementTree as ET
    import os
    import random

    tenant_url = os.environ.get('TENANT_URL')
    refresh_token = os.environ.get('REFRESH_TOKEN')

    def auth_vcloud(refreshToken):
        conn = http.client.HTTPSConnection(tenant_url)
        payload = 'grant_type=refresh_token&refresh_token='+refreshToken
        headers = {
        'Accept': 'application/json',
        'Content-Type': 'application/x-www-form-urlencoded'
        }
        conn.request("POST", "/oauth/tenant/C0000001-004/token", payload, headers)
        res = conn.getresponse()
        data = res.read()
        jsondata = json.loads(data.decode("utf-8"))
        return jsondata["access_token"]

    def node_name_to_vmId(access_token, node_name):
        conn = http.client.HTTPSConnection(tenant_url)
        payload = ''
        headers = {
        'Accept': '*/*;version=36.2',
        'Authorization': 'Bearer '+ access_token
        }
        conn.request("GET", "/api/vms/query?pageSize=128", payload, headers)
        res = conn.getresponse()
        data = res.read()
        root = ET.fromstring(data)
        for vmrecord in root.findall('{http://www.vmware.com/vcloud/v1.5}VMRecord'):
            href = vmrecord.get('href')
            name = vmrecord.get('name')
            if name == node_name:
                vm_id = href.removeprefix('https://'+tenant_url+'/api/vApp/')
                return vm_id

    def node_reboot(access_token, vm_id):
        conn = http.client.HTTPSConnection(tenant_url)
        payload = ''
        headers = {
        'Accept': '*/*;version=36.2',
        'Authorization': 'Bearer '+ access_token
        }
        conn.request("POST", "/api/vApp/"+vm_id+"/power/action/reboot", payload, headers)
        res = conn.getresponse()
        data = res.read()
        print(data.decode("utf-8"))
    
    def run(cmd):
        try:
            output = subprocess.Popen(
                cmd, shell=True, universal_newlines=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT
            )
            (out, err) = output.communicate()
        except Exception as e:
            logging.error("Failed to run %s, error: %s" % (cmd, e))
        return out

    worker_nodes = run("oc get nodes -l node-role.kubernetes.io/infra='' -o custom-columns=NODE:.metadata.name --no-headers ").rstrip()
    workers = worker_nodes.split('\n')
    random_worker_index = random.randint(0,2)
    stop_kublet = run("oc debug node/" + workers[random_worker_index] + " -- chroot /host systemctl stop kubelet").rstrip()
    node_reboot(auth_vcloud(refresh_token), node_name_to_vmId(auth_vcloud(refresh_token), workers[random_worker_index]))
  startx_node_worker.yaml: |
      - id: run_python
        config:
          filename: scenarios/startx_node_worker.py
  startx_node_worker.py: |
    #!/usr/bin/env python3
    import subprocess
    import logging
    import time
    import http.client
    import json
    import xml.etree.ElementTree as ET
    import os
    import random

    tenant_url = os.environ.get('TENANT_URL')
    refresh_token = os.environ.get('REFRESH_TOKEN')

    def auth_vcloud(refreshToken):
        conn = http.client.HTTPSConnection(tenant_url)
        payload = 'grant_type=refresh_token&refresh_token='+refreshToken
        headers = {
        'Accept': 'application/json',
        'Content-Type': 'application/x-www-form-urlencoded'
        }
        conn.request("POST", "/oauth/tenant/C0000001-004/token", payload, headers)
        res = conn.getresponse()
        data = res.read()
        jsondata = json.loads(data.decode("utf-8"))
        return jsondata["access_token"]

    def node_name_to_vmId(access_token, node_name):
        conn = http.client.HTTPSConnection(tenant_url)
        payload = ''
        headers = {
        'Accept': '*/*;version=36.2',
        'Authorization': 'Bearer '+ access_token
        }
        conn.request("GET", "/api/vms/query?pageSize=128", payload, headers)
        res = conn.getresponse()
        data = res.read()
        root = ET.fromstring(data)
        for vmrecord in root.findall('{http://www.vmware.com/vcloud/v1.5}VMRecord'):
            href = vmrecord.get('href')
            name = vmrecord.get('name')
            if name == node_name:
                vm_id = href.removeprefix('https://'+tenant_url+'/api/vApp/')
                return vm_id

    def node_reboot(access_token, vm_id):
        conn = http.client.HTTPSConnection(tenant_url)
        payload = ''
        headers = {
        'Accept': '*/*;version=36.2',
        'Authorization': 'Bearer '+ access_token
        }
        conn.request("POST", "/api/vApp/"+vm_id+"/power/action/reboot", payload, headers)
        res = conn.getresponse()
        data = res.read()
        print(data.decode("utf-8"))

    def run(cmd):
        try:
            output = subprocess.Popen(
                cmd, shell=True, universal_newlines=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT
            )
            (out, err) = output.communicate()
        except Exception as e:
            logging.error("Failed to run %s, error: %s" % (cmd, e))
        return out

    worker_nodes = run("oc get nodes -l node-role.kubernetes.io/worker='' -o custom-columns=NODE:.metadata.name --no-headers").rstrip()
    workers = worker_nodes.split('\n')
    random_worker_index = random.randint(0,2)
    stop_kublet = run("oc debug node/" + workers[random_worker_index] + " -- chroot /host systemctl stop kubelet").rstrip()
    node_reboot(auth_vcloud(refresh_token), node_name_to_vmId(auth_vcloud(refresh_token), workers[random_worker_index]))
  startx_node_app.yaml: |
    config:
      runStrategy:
        runs: 1
        maxSecondsBetweenRuns: 40
        minSecondsBetweenRuns: 20
    node_scenarios:
      - actions:
          - stop_kubelet_scenario
          - start_kubelet_scenario
        node_name:
        label_selector: node-role.kubernetes.io/worker
        instance_count: 1
        runs: 1
        timeout: 300 
        cloud_type: bm 
      # - actions:
      #     - node_crash_scenario
      #   node_name:
      #   label_selector: node-role.kubernetes.io/worker
      #   instance_count: 1
      #   runs: 1
      #   timeout: 300 
      #   cloud_type: generic 
      # - actions: 
      #     - node_start_scenario
      #     - node_stop_start_scenario
      #     - node_termination_scenario
      #     - node_reboot_scenario
      #     - stop_kubelet_scenario
      #     - stop_start_kubelet_scenario
      #     - node_crash_scenario
      #     - stop_start_helper_node_scenario
      #   node_name:
      #   label_selector: node-role.kubernetes.io/worker
      #   instance_count: 1
      #   runs: 1
      #   timeout: 300 
      #   cloud_type: aws 
      # - actions:
      #   - node_reboot_scenario
      #   node_name:
      #   label_selector: node-role.kubernetes.io/worker
      #   instance_count: 1
      #   timeout: 300
      #   cloud_type: aws
  startx_openshift-apiserver.yml: |
    - id: kill-pods
      config:
        namespace_pattern: ^openshift-apiserver$
        label_selector: app=openshift-apiserver-a
    - id: wait-for-pods
      config:
        namespace_pattern: ^openshift-apiserver$
        label_selector: app=openshift-apiserver-a
        count: 3
  startx_openshift_etcd_container.yml: |
    scenarios:
    - name: "kill etcd container"
      namespace: "openshift-etcd"
      label_selector: "k8s-app=etcd"
      container_name: "etcd"
      action: "kill 1"
      count: 1
      expected_recovery_time: 60
  startx_openshift_etcd_pod.yml: |
    - id: kill-pods
      config:
        namespace_pattern: ^openshift-etcd$
        label_selector: k8s-app=etcd
    - id: wait-for-pods
      config:
        namespace_pattern: ^openshift-etcd$
        label_selector: k8s-app=etcd
        count: 3
  startx_openshift_logging.yml: |
    - id: kill-pods
      config:
        namespace_pattern: ^openshift-logging$
        label_selector: component=collector
        kill: 2

    - id: kill-pods
      config:
        namespace_pattern: ^openshift-logging$
        label_selector: component=elasticsearch
        kill: 1

    - id: kill-pods
      config:
        namespace_pattern: ^openshift-logging$
        name_pattern: .*
        kill: 5
  startx_openshift_machines.yml: |
    config:
      runStrategy:
        runs: 1
        maxSecondsBetweenRuns: 10
        minSecondsBetweenRuns: 5
    scenarios:
      - name: kill 1 server pod in openshift-machine-config-operator namespace
        steps:
        - podAction:
            matches:
              - namespace: "openshift-machine-config-operator"
              - labels:
                  namespace: "openshift-machine-config-operator"
                  selector: "k8s-app=machine-config-server"
            filters:
              - property:
                  name: "state"
                  value: "Running"
              - randomSample:
                  size: 1
            actions:
              - kill:
                  probability: 1
                  force: true
      - name: kill up to 3 daemon running pods in openshift-machine-config-operator namespace
        steps:
        - podAction:
            matches:
              - namespace: "openshift-machine-config-operator"
              - labels:
                  namespace: "openshift-machine-config-operator"
                  selector: "k8s-app=machine-config-daemon"
            filters:
              - property:
                  name: "state"
                  value: "Running"
              - randomSample:
                  size: 3
            actions:
              - kill:
                  probability: .75
      - name: kill up to 4 running pods in openshift-machine-* namespace
        steps:
        - podAction:
            matches:
              - namespace: "openshift-machine-*"
            filters:
              - property:
                  name: "state"
                  value: "Running"
              - randomSample:
                  size: 4
            actions:
              - kill:
                  probability: .75
      - name: kill 1 pod in openshift-machine-api namespace
        steps:
        - podAction:
            matches:
              - namespace: "openshift-machine-api"
            filters:
              - property:
                  name: "state"
                  value: "Running"
              - randomSample:
                  size: 1
            actions:
              - kill:
                  probability: .7
  startx_openshift_monitoring.yml: |
    - id: kill-pods
      config:
        namespace_pattern: ^openshift-monitoring$
        label_selector: app=prometheus
    - id: wait-for-pods
      config:
        namespace_pattern: ^openshift-monitoring$
        label_selector: app=prometheus
        count: 2
        timeout: 180

    - id: kill-pods
      config:
        namespace_pattern: ^openshift-monitoring$
        label_selector: "app.kubernetes.io/name=node-exporter"
        kill: 2
    - id: wait-for-pods
      config:
        namespace_pattern: ^openshift-monitoring$
        label_selector: "app.kubernetes.io/name=node-exporter"
        count: 8
        timeout: 180

    - id: kill-pods
      config:
        namespace_pattern: ^openshift-monitoring$
        label_selector: "app.kubernetes.io/name=alertmanager"
    - id: wait-for-pods
      config:
        namespace_pattern: ^openshift-monitoring$
        label_selector: "app.kubernetes.io/name=alertmanager"
        count: 2
        timeout: 180

    - id: kill-pods
      config:
        namespace_pattern: ^openshift-monitoring$
        label_selector: "app.kubernetes.io/name=thanos-query"
    - id: wait-for-pods
      config:
        namespace_pattern: ^openshift-monitoring$
        label_selector: "app.kubernetes.io/name=thanos-query"
        count: 2
        timeout: 180

    - id: kill-pods
      config:
        namespace_pattern: ^openshift-monitoring$
        name_pattern: .*
        kill: 5
  startx_openshift_acm.yml: |
    - id: kill-pods
      config:
        namespace_pattern: ^open-cluster-management$
        label_selector: app=cluster-manager
    - id: wait-for-pods
      config:
        namespace_pattern: ^open-cluster-management$
        label_selector: app=cluster-manager
        timeout: 180

    - id: kill-pods
      config:
        namespace_pattern: ^open-cluster-management$
        label_selector: app=application-chart
        kill: 2
    - id: wait-for-pods
      config:
        namespace_pattern: ^open-cluster-management$
        label_selector: app=application-chart
        timeout: 180

    - id: kill-pods
      config:
        namespace_pattern: ^open-cluster-management$
        label_selector: app=klusterlet-addon-controller-v2
    - id: wait-for-pods
      config:
        namespace_pattern: ^open-cluster-management$
        label_selector: app=klusterlet-addon-controller-v2
        timeout: 180

    - id: kill-pods
      config:
        namespace_pattern: ^open-cluster-management$
        label_selector: app=ocm-controller
    - id: wait-for-pods
      config:
        namespace_pattern: ^open-cluster-management$
        label_selector: app=ocm-controller
        timeout: 180

    - id: kill-pods
      config:
        namespace_pattern: ^open-cluster-management$
        name_pattern: .*
        kill: 5
    - id: wait-for-pods
      config:
        namespace_pattern: ^open-cluster-management$
        timeout: 180
  startx_openshift_acs.yml: |
    - id: kill-pods
      config:
        namespace_pattern: ^stackrox$
        label_selector: app=collector
        kill: 5
    - id: wait-for-pods
      config:
        namespace_pattern: ^stackrox$
        label_selector: app=collector
        timeout: 180

    - id: kill-pods
      config:
        namespace_pattern: ^stackrox$
        label_selector: app=admission-control
        kill: 2
    - id: wait-for-pods
      config:
        namespace_pattern: ^stackrox$
        label_selector: app=admission-control
        timeout: 180

    - id: kill-pods
      config:
        namespace_pattern: ^stackrox$
        label_selector: app=scanner
    - id: wait-for-pods
      config:
        namespace_pattern: ^stackrox$
        label_selector: app=scanner
        timeout: 180

    - id: kill-pods
      config:
        namespace_pattern: ^stackrox$
        name_pattern: .*
        kill: 5
    - id: wait-for-pods
      config:
        namespace_pattern: ^stackrox$
        timeout: 180
  startx_openshift_podmonkey.yml: |
    - id: kill-pods
      config:
        namespace_pattern: ^openshift-.*$
        name_pattern: .*
        kill: 5
  startx_time_node.yml: |
    time_scenarios:
      - action: skew_date
        object_type: node
        label_selector: node-role.kubernetes.io/master
      - action: skew_date
        object_type: node
        label_selector: node-role.kubernetes.io/infra
      - action: skew_date
        object_type: node
        label_selector: node-role.kubernetes.io/worker
      - action: skew_time
        object_type: node
        label_selector: node-role.kubernetes.io/master
      - action: skew_time
        object_type: node
        label_selector: node-role.kubernetes.io/infra
      - action: skew_time
        object_type: node
        label_selector: node-role.kubernetes.io/worker
  startx_time_pod.yml: |
    time_scenarios:
      - action: skew_date
        object_type: pod
        namespace: openshift-etcd
        container_name: etcd
        label_selector: app=etcd
      - action: skew_date
        object_type: pod
        namespace: openshift-machine-config-operator
        container_name: 
        label_selector: "k8s-app=machine-config-daemon"
      - action: skew_date
        object_type: pod
        namespace: openshift-apiserver
        container_name: 
        label_selector: "app=openshift-apiserver-a"
      - action: skew_time
        object_type: pod
        namespace: openshift-etcd
        container_name: etcd
        label_selector: app=etcd
      - action: skew_time
        object_type: pod
        namespace: openshift-machine-config-operator
        container_name: 
        label_selector: "k8s-app=machine-config-daemon"
      - action: skew_time
        object_type: pod
        namespace: openshift-apiserver
        container_name: 
        label_selector: "app=openshift-apiserver-a"
  startx_zone_outage.yaml: |
    zone_outage:
      cloud_type: aws
      duration: 120
      vpc_id: vpc-0a06fb82c7deef947
      subnet_id: ["subnet-08c7f41c25bc51eb9"]
      # subnet_id: ["subnet-08c7f41c25bc51eb9", "subnet-04441df42fd95bfbd", "subnet-092a12092c3205e17"]
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kraken-kubeconfig
  namespace: "{{- $namespace -}}"
  labels:
    app.kubernetes.io/name: "kraken-kubeconfig"
    {{- include "chaos-kraken.labels" $root | nindent 4 }}
  annotations:
    {{- include "chaos-kraken.annotations" $root | nindent 4 }}
data:
{{- if eq .Values.kraken.kubeconfig.mode "file" }}
  config: |
    {{- .Values.kraken.kubeconfig.file | nindent 4 }}
{{- end }}
{{- if eq .Values.kraken.kubeconfig.mode "token" }}
  config: |
    kind: Config
    apiVersion: v1
    current-context: default
    clusters:
    - name: default
      cluster:
        insecure-skip-tls-verify: true
        server: "{{ .Values.kraken.kubeconfig.token.server }}"
    users:
    - name: default
      user:
        token: "{{ .Values.kraken.kubeconfig.token.token }}"
    contexts:
    - name: default
      context:
        cluster: default
        namespace: default
        user: default
    preferences: {}
{{- end }}
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kraken-config-chaos-logging
  namespace: "{{- $namespace -}}"
  labels:
    app.kubernetes.io/name: "kraken-config-chaos-logging"
    {{- include "chaos-kraken.labels" $root | nindent 4 }}
  annotations:
    {{- include "chaos-kraken.annotations" $root | nindent 4 }}
data:
  config.yaml: |-
    kraken:
      distribution: openshift
      kubeconfig_path: /root/.kube/config
      exit_on_failure: False
      port: 8081
      publish_kraken_status: False
      signal_address: 0.0.0.0
      signal_state: RUN
      litmus_version: v1.13.6
      litmus_uninstall: False
      litmus_uninstall_before_run: False
      chaos_scenarios:
        - plugin_scenarios:
          - scenarios/startx_openshift_logging.yml
    cerberus:
        cerberus_enabled: True
        cerberus_url: {{ $root.Values.kraken.cerberusUrl | quote }}
        check_applicaton_routes: True
    performance_monitoring:
        deploy_dashboards: False
        repo: "https://github.com/cloud-bulldozer/performance-dashboards.git"
        kube_burner_binary_url: "https://github.com/cloud-bulldozer/kube-burner/releases/download/v1.7.2/kube-burner-V1.7.2-linux-x86_64.tar.gz"
        capture_metrics: True
        config_path: config/kube_burner.yaml
        metrics_profile_path: config/metrics-aggregated.yaml
        prometheus_url:
        prometheus_bearer_token:
        uuid:
        enable_alerts: False
        alert_profile: config/alerts
    tunings:
        wait_duration: 180
        iterations: 1
        daemon_mode: False
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kraken-config-chaos-acm
  namespace: "{{- $namespace -}}"
  labels:
    app.kubernetes.io/name: "kraken-config-chaos-acm"
    {{- include "chaos-kraken.labels" $root | nindent 4 }}
  annotations:
    {{- include "chaos-kraken.annotations" $root | nindent 4 }}
data:
  config.yaml: |-
    kraken:
      distribution: openshift
      kubeconfig_path: /root/.kube/config
      exit_on_failure: False
      port: 8081
      publish_kraken_status: False
      signal_address: 0.0.0.0
      signal_state: RUN
      litmus_version: v1.13.6
      litmus_uninstall: False
      litmus_uninstall_before_run: False
      chaos_scenarios:
        - plugin_scenarios:
          - scenarios/startx_openshift_acm.yml
    cerberus:
        cerberus_enabled: True
        cerberus_url: {{ $root.Values.kraken.cerberusUrl | quote }}
        check_applicaton_routes: True
    performance_monitoring:
        deploy_dashboards: False
        repo: "https://github.com/cloud-bulldozer/performance-dashboards.git"
        kube_burner_binary_url: "https://github.com/cloud-bulldozer/kube-burner/releases/download/v1.7.2/kube-burner-V1.7.2-linux-x86_64.tar.gz"
        capture_metrics: True
        config_path: config/kube_burner.yaml
        metrics_profile_path: config/metrics-aggregated.yaml
        prometheus_url:
        prometheus_bearer_token:
        uuid:
        enable_alerts: False
        alert_profile: config/alerts
    tunings:
        wait_duration: 180
        iterations: 1
        daemon_mode: False
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kraken-config-chaos-acs
  namespace: "{{- $namespace -}}"
  labels:
    app.kubernetes.io/name: "kraken-config-chaos-acm"
    {{- include "chaos-kraken.labels" $root | nindent 4 }}
  annotations:
    {{- include "chaos-kraken.annotations" $root | nindent 4 }}
data:
  config.yaml: |-
    kraken:
      distribution: openshift
      kubeconfig_path: /root/.kube/config
      exit_on_failure: False
      port: 8081
      publish_kraken_status: False
      signal_address: 0.0.0.0
      signal_state: RUN
      litmus_version: v1.13.6
      litmus_uninstall: False
      litmus_uninstall_before_run: False
      chaos_scenarios:
        - plugin_scenarios:
          - scenarios/startx_openshift_acs.yml
    cerberus:
        cerberus_enabled: True
        cerberus_url: {{ $root.Values.kraken.cerberusUrl | quote }}
        check_applicaton_routes: True
    performance_monitoring:
        deploy_dashboards: False
        repo: "https://github.com/cloud-bulldozer/performance-dashboards.git"
        kube_burner_binary_url: "https://github.com/cloud-bulldozer/kube-burner/releases/download/v1.7.2/kube-burner-V1.7.2-linux-x86_64.tar.gz"
        capture_metrics: True
        config_path: config/kube_burner.yaml
        metrics_profile_path: config/metrics-aggregated.yaml
        prometheus_url:
        prometheus_bearer_token:
        uuid:
        enable_alerts: False
        alert_profile: config/alerts
    tunings:
        wait_duration: 180
        iterations: 1
        daemon_mode: False
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kraken-config-chaos-monitoring
  namespace: "{{- $namespace -}}"
  labels:
    app.kubernetes.io/name: "kraken-config-chaos-monitoring"
    {{- include "chaos-kraken.labels" $root | nindent 4 }}
  annotations:
    {{- include "chaos-kraken.annotations" $root | nindent 4 }}
data:
  config.yaml: |-
    kraken:
      distribution: openshift
      kubeconfig_path: /root/.kube/config
      exit_on_failure: False
      port: 8081
      publish_kraken_status: False
      signal_address: 0.0.0.0
      signal_state: RUN
      litmus_version: v1.13.6
      litmus_uninstall: False
      litmus_uninstall_before_run: False
      chaos_scenarios:
        - plugin_scenarios:
          - scenarios/startx_openshift_monitoring.yml
    cerberus:
        cerberus_enabled: True
        cerberus_url: {{ $root.Values.kraken.cerberusUrl | quote }}
        check_applicaton_routes: True
    performance_monitoring:
        deploy_dashboards: False
        repo: "https://github.com/cloud-bulldozer/performance-dashboards.git"
        kube_burner_binary_url: "https://github.com/cloud-bulldozer/kube-burner/releases/download/v1.7.2/kube-burner-V1.7.2-linux-x86_64.tar.gz"
        capture_metrics: True
        config_path: config/kube_burner.yaml
        metrics_profile_path: config/metrics-aggregated.yaml
        prometheus_url:
        prometheus_bearer_token:
        uuid:
        enable_alerts: False
        alert_profile: config/alerts
    tunings:
        wait_duration: 180
        iterations: 1
        daemon_mode: False
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kraken-config-chaos-machines
  namespace: "{{- $namespace -}}"
  labels:
    app.kubernetes.io/name: "kraken-config-chaos-machines"
    {{- include "chaos-kraken.labels" $root | nindent 4 }}
  annotations:
    {{- include "chaos-kraken.annotations" $root | nindent 4 }}
data:
  config.yaml: |-
    kraken:
      distribution: openshift
      kubeconfig_path: /root/.kube/config
      exit_on_failure: False
      port: 8081
      publish_kraken_status: False
      signal_address: 0.0.0.0
      signal_state: RUN
      litmus_version: v1.13.6
      litmus_uninstall: False
      litmus_uninstall_before_run: False
      chaos_scenarios:
        - plugin_scenarios:
          - scenarios/startx_openshift_machines.yml
    cerberus:
        cerberus_enabled: True
        cerberus_url: {{ $root.Values.kraken.cerberusUrl | quote }}
        check_applicaton_routes: True
    performance_monitoring:
        deploy_dashboards: False
        repo: "https://github.com/cloud-bulldozer/performance-dashboards.git"
        kube_burner_binary_url: "https://github.com/cloud-bulldozer/kube-burner/releases/download/v1.7.2/kube-burner-V1.7.2-linux-x86_64.tar.gz"
        capture_metrics: True
        config_path: config/kube_burner.yaml
        metrics_profile_path: config/metrics-aggregated.yaml
        prometheus_url:
        prometheus_bearer_token:
        uuid:
        enable_alerts: False
        alert_profile: config/alerts
    tunings:
        wait_duration: 300
        iterations: 1
        daemon_mode: False
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kraken-config-chaos-apiserver
  namespace: "{{- $namespace -}}"
  labels:
    app.kubernetes.io/name: "kraken-config-chaos-apiserver"
    {{- include "chaos-kraken.labels" $root | nindent 4 }}
  annotations:
    {{- include "chaos-kraken.annotations" $root | nindent 4 }}
data:
  config.yaml: |-
    kraken:
      distribution: openshift
      kubeconfig_path: /root/.kube/config
      exit_on_failure: False
      port: 8081
      publish_kraken_status: False
      signal_address: 0.0.0.0
      signal_state: RUN
      litmus_version: v1.13.6
      litmus_uninstall: False
      litmus_uninstall_before_run: False
      chaos_scenarios:
        - plugin_scenarios:
          - scenarios/startx_openshift-apiserver.yml
    cerberus:
        cerberus_enabled: True
        cerberus_url: {{ $root.Values.kraken.cerberusUrl | quote }}
        check_applicaton_routes: True
    performance_monitoring:
        deploy_dashboards: False
        repo: "https://github.com/cloud-bulldozer/performance-dashboards.git"
        kube_burner_binary_url: "https://github.com/cloud-bulldozer/kube-burner/releases/download/v1.7.2/kube-burner-V1.7.2-linux-x86_64.tar.gz"
        capture_metrics: True
        config_path: config/kube_burner.yaml
        metrics_profile_path: config/metrics-aggregated.yaml
        prometheus_url:
        prometheus_bearer_token:
        uuid:
        enable_alerts: False
        alert_profile: config/alerts
    tunings:
        wait_duration: 180
        iterations: 1
        daemon_mode: False
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kraken-config-chaos-etcd
  namespace: "{{- $namespace -}}"
  labels:
    app.kubernetes.io/name: "kraken-config-chaos-etcd"
    {{- include "chaos-kraken.labels" $root | nindent 4 }}
  annotations:
    {{- include "chaos-kraken.annotations" $root | nindent 4 }}
data:
  config.yaml: |-
    kraken:
      distribution: openshift
      kubeconfig_path: /root/.kube/config
      exit_on_failure: False
      port: 8081
      publish_kraken_status: False
      signal_address: 0.0.0.0
      signal_address: 0.0.0.0
      signal_state: RUN
      litmus_version: v1.13.6
      litmus_uninstall: False
      litmus_uninstall_before_run: False
      chaos_scenarios:
        - plugin_scenarios:
          - scenarios/startx_openshift_etcd_pod.yml
        - container_scenarios: 
          - - scenarios/startx_openshift_etcd_container.yml
    cerberus:
        cerberus_enabled: True
        cerberus_url: {{ $root.Values.kraken.cerberusUrl | quote }}
        check_applicaton_routes: True
    performance_monitoring:
        deploy_dashboards: False
        repo: "https://github.com/cloud-bulldozer/performance-dashboards.git"
        kube_burner_binary_url: "https://github.com/cloud-bulldozer/kube-burner/releases/download/v1.7.2/kube-burner-V1.7.2-linux-x86_64.tar.gz"
        capture_metrics: True
        config_path: config/kube_burner.yaml
        metrics_profile_path: config/metrics-aggregated.yaml
        prometheus_url:
        prometheus_bearer_token:
        uuid:
        enable_alerts: False
        alert_profile: config/alerts
    tunings:
        wait_duration: 480
        iterations: 1
        daemon_mode: False
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kraken-config-chaos-releasemonkey
  namespace: "{{- $namespace -}}"
  labels:
    app.kubernetes.io/name: "kraken-config-chaos-releasemonkey"
    {{- include "chaos-kraken.labels" $root | nindent 4 }}
  annotations:
    {{- include "chaos-kraken.annotations" $root | nindent 4 }}
data:
  config.yaml: |-
    kraken:
      distribution: openshift
      kubeconfig_path: /root/.kube/config
      exit_on_failure: False
      port: 8081
      publish_kraken_status: False
      signal_address: 0.0.0.0
      signal_state: RUN
      litmus_version: v1.13.6
      litmus_uninstall: False
      litmus_uninstall_before_run: False
      chaos_scenarios:
        - plugin_scenarios:
          - scenarios/startx_openshift_podmonkey.yml
    cerberus:
        cerberus_enabled: True
        cerberus_url: {{ $root.Values.kraken.cerberusUrl | quote }}
        check_applicaton_routes: True
    performance_monitoring:
        deploy_dashboards: False
        repo: "https://github.com/cloud-bulldozer/performance-dashboards.git"
        kube_burner_binary_url: "https://github.com/cloud-bulldozer/kube-burner/releases/download/v1.7.2/kube-burner-V1.7.2-linux-x86_64.tar.gz"
        capture_metrics: True
        config_path: config/kube_burner.yaml
        metrics_profile_path: config/metrics-aggregated.yaml
        prometheus_url:
        prometheus_bearer_token:
        uuid:
        enable_alerts: False
        alert_profile: config/alerts
    tunings:
        wait_duration: 240
        iterations: 2
        daemon_mode: False
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kraken-config-chaos-time
  namespace: "{{- $namespace -}}"
  labels:
    app.kubernetes.io/name: "kraken-config-chaos-time"
    {{- include "chaos-kraken.labels" $root | nindent 4 }}
  annotations:
    {{- include "chaos-kraken.annotations" $root | nindent 4 }}
data:
  config.yaml: |-
    kraken:
      distribution: openshift
      kubeconfig_path: /root/.kube/config
      exit_on_failure: False
      port: 8081
      publish_kraken_status: False
      signal_address: 0.0.0.0
      signal_state: RUN
      litmus_version: v1.13.6
      litmus_uninstall: False
      litmus_uninstall_before_run: False
      chaos_scenarios:
        - time_scenarios: 
          - scenarios/startx_time_pod.yml
          - scenarios/startx_time_node.yml
    cerberus:
        cerberus_enabled: True
        cerberus_url: {{ $root.Values.kraken.cerberusUrl | quote }}
        check_applicaton_routes: True
    performance_monitoring:
        deploy_dashboards: False
        repo: "https://github.com/cloud-bulldozer/performance-dashboards.git"
        kube_burner_binary_url: "https://github.com/cloud-bulldozer/kube-burner/releases/download/v1.7.2/kube-burner-V1.7.2-linux-x86_64.tar.gz"
        capture_metrics: True
        config_path: config/kube_burner.yaml
        metrics_profile_path: config/metrics-aggregated.yaml
        prometheus_url:
        prometheus_bearer_token:
        uuid:
        enable_alerts: False
        alert_profile: config/alerts
    tunings:
        wait_duration: 420
        iterations: 1
        daemon_mode: False
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kraken-config-chaos-nodes
  namespace: "{{- $namespace -}}"
  labels:
    app.kubernetes.io/name: "kraken-config-chaos-nodes"
    {{- include "chaos-kraken.labels" $root | nindent 4 }}
  annotations:
    {{- include "chaos-kraken.annotations" $root | nindent 4 }}
data:
  config.yaml: |-
    kraken:
      distribution: openshift
      kubeconfig_path: /root/.kube/config
      exit_on_failure: False
      port: 8081
      publish_kraken_status: False
      signal_address: 0.0.0.0
      signal_state: RUN
      litmus_version: v1.13.6
      litmus_uninstall: False
      litmus_uninstall_before_run: False
      chaos_scenarios:
        - plugin_scenarios:
          - scenarios/startx_node_master.yaml
          - scenarios/startx_node_infra.yaml
          - scenarios/startx_node_worker.yaml
    cerberus:
        cerberus_enabled: True
        cerberus_url: {{ $root.Values.kraken.cerberusUrl | quote }}
        check_applicaton_routes: True
    performance_monitoring:
        deploy_dashboards: False
        repo: "https://github.com/cloud-bulldozer/performance-dashboards.git"
        kube_burner_binary_url: "https://github.com/cloud-bulldozer/kube-burner/releases/download/v1.7.2/kube-burner-V1.7.2-linux-x86_64.tar.gz"
        capture_metrics: True
        config_path: config/kube_burner.yaml
        metrics_profile_path: config/metrics-aggregated.yaml
        prometheus_url:
        prometheus_bearer_token:
        uuid:
        enable_alerts: False
        alert_profile: config/alerts
    tunings:
        wait_duration: 300
        iterations: 1
        daemon_mode: False
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kraken-config-chaos-network
  namespace: "{{- $namespace -}}"
  labels:
    app.kubernetes.io/name: "kraken-config-chaos-network"
    {{- include "chaos-kraken.labels" $root | nindent 4 }}
  annotations:
    {{- include "chaos-kraken.annotations" $root | nindent 4 }}
data:
  config.yaml: |-
    kraken:
      distribution: openshift
      kubeconfig_path: /root/.kube/config
      exit_on_failure: False
      port: 8081
      publish_kraken_status: False
      signal_address: 0.0.0.0
      signal_state: RUN
      litmus_version: v1.13.6
      litmus_uninstall: False
      litmus_uninstall_before_run: False
      chaos_scenarios:
        - network_chaos:
          # - scenarios/startx_node_app_network_chaos.yaml
          - scenarios/startx_node_infra_network_chaos.yaml
          # - scenarios/startx_node_master_network_chaos.yaml
    cerberus:
        cerberus_enabled: True
        cerberus_url: {{ $root.Values.kraken.cerberusUrl | quote }}
        check_applicaton_routes: True
    performance_monitoring:
        deploy_dashboards: False
        repo: "https://github.com/cloud-bulldozer/performance-dashboards.git"
        kube_burner_binary_url: "https://github.com/cloud-bulldozer/kube-burner/releases/download/v1.7.2/kube-burner-V1.7.2-linux-x86_64.tar.gz"
        capture_metrics: True
        config_path: config/kube_burner.yaml
        metrics_profile_path: config/metrics-aggregated.yaml
        prometheus_url:
        prometheus_bearer_token:
        uuid:
        enable_alerts: False
        alert_profile: config/alerts
    tunings:
        wait_duration: 300
        iterations: 1
        daemon_mode: False
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kraken-config-chaos-zone
  namespace: "{{- $namespace -}}"
  labels:
    app.kubernetes.io/name: "kraken-config-chaos-zone"
    {{- include "chaos-kraken.labels" $root | nindent 4 }}
  annotations:
    {{- include "chaos-kraken.annotations" $root | nindent 4 }}
data:
  config.yaml: |-
    kraken:
      distribution: openshift
      kubeconfig_path: /root/.kube/config
      exit_on_failure: False
      port: 8081
      publish_kraken_status: False
      signal_address: 0.0.0.0
      signal_state: RUN
      litmus_version: v1.13.6
      litmus_uninstall: False
      litmus_uninstall_before_run: False
      chaos_scenarios:
        - zone_outages:
          - scenarios/startx_zone_outage.yaml
    cerberus:
        cerberus_enabled: True
        cerberus_url: {{ $root.Values.kraken.cerberusUrl | quote }}
        check_applicaton_routes: True
    performance_monitoring:
        deploy_dashboards: False
        repo: "https://github.com/cloud-bulldozer/performance-dashboards.git"
        kube_burner_binary_url: "https://github.com/cloud-bulldozer/kube-burner/releases/download/v1.7.2/kube-burner-V1.7.2-linux-x86_64.tar.gz"
        capture_metrics: True
        config_path: config/kube_burner.yaml
        metrics_profile_path: config/metrics-aggregated.yaml
        prometheus_url:
        prometheus_bearer_token:
        uuid:
        enable_alerts: False
        alert_profile: config/alerts
    tunings:
        wait_duration: 600
        iterations: 1
        daemon_mode: False
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kraken-config-chaos-storage
  namespace: "{{- $namespace -}}"
  labels:
    app.kubernetes.io/name: "kraken-config-chaos-storage"
    {{- include "chaos-kraken.labels" $root | nindent 4 }}
  annotations:
    {{- include "chaos-kraken.annotations" $root | nindent 4 }}
data:
  config.yaml: |-
    kraken:
      distribution: openshift
      kubeconfig_path: /root/.kube/config
      exit_on_failure: False
      port: 8081
      publish_kraken_status: False
      signal_address: 0.0.0.0
      signal_state: RUN
      litmus_version: v1.13.6
      litmus_uninstall: False
      litmus_uninstall_before_run: False
      chaos_scenarios:
        - pvc_scenarios:
          - scenarios/startx_pvc_fruitapp1-preprod-postgresql.yaml
          - scenarios/startx_pvc_fruitapp2-prod-postgresql.yaml
    cerberus:
        cerberus_enabled: True
        cerberus_url: {{ $root.Values.kraken.cerberusUrl | quote }}
        check_applicaton_routes: True
    performance_monitoring:
        deploy_dashboards: False
        repo: "https://github.com/cloud-bulldozer/performance-dashboards.git"
        kube_burner_binary_url: "https://github.com/cloud-bulldozer/kube-burner/releases/download/v1.7.2/kube-burner-V1.7.2-linux-x86_64.tar.gz"
        capture_metrics: True
        config_path: config/kube_burner.yaml
        metrics_profile_path: config/metrics-aggregated.yaml
        prometheus_url:
        prometheus_bearer_token:
        uuid:
        enable_alerts: False
        alert_profile: config/alerts
    tunings:
        wait_duration: 60
        iterations: 1
        daemon_mode: False
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kraken-config-chaos-cluster
  namespace: "{{- $namespace -}}"
  labels:
    app.kubernetes.io/name: "kraken-config-chaos-cluster"
    {{- include "chaos-kraken.labels" $root | nindent 4 }}
  annotations:
    {{- include "chaos-kraken.annotations" $root | nindent 4 }}
data:
  config.yaml: |-
    kraken:
      distribution: openshift
      kubeconfig_path: /root/.kube/config
      exit_on_failure: False
      port: 8081
      publish_kraken_status: False
      signal_address: 0.0.0.0
      signal_state: RUN
      litmus_version: v1.13.6
      litmus_uninstall: False
      litmus_uninstall_before_run: False
      chaos_scenarios:
        - plugin_scenarios:
          - scenarios/cluster_shut_down_scenario.yaml
    cerberus:
        cerberus_enabled: True
        cerberus_url: {{ $root.Values.kraken.cerberusUrl | quote }}
        check_applicaton_routes: True
    performance_monitoring:
        deploy_dashboards: False
        repo: "https://github.com/cloud-bulldozer/performance-dashboards.git"
        kube_burner_binary_url: "https://github.com/cloud-bulldozer/kube-burner/releases/download/v1.7.2/kube-burner-V1.7.2-linux-x86_64.tar.gz"
        capture_metrics: True
        config_path: config/kube_burner.yaml
        metrics_profile_path: config/metrics-aggregated.yaml
        prometheus_url:
        prometheus_bearer_token:
        uuid:
        enable_alerts: False
        alert_profile: config/alerts
    tunings:
        wait_duration: 600
        iterations: 1
        daemon_mode: False
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kraken-config-all
  namespace: "{{- $namespace -}}"
  labels:
    app.kubernetes.io/name: "kraken-config-all"
    {{- include "chaos-kraken.labels" $root | nindent 4 }}
  annotations:
    {{- include "chaos-kraken.annotations" $root | nindent 4 }}
data:
  config.yaml: |-
    kraken:
      distribution: openshift
      kubeconfig_path: /root/.kube/config
      exit_on_failure: False
      port: 8081
      publish_kraken_status: False
      signal_address: 0.0.0.0
      signal_state: RUN
      litmus_version: v1.13.6
      litmus_uninstall: False
      litmus_uninstall_before_run: False
      chaos_scenarios:
        - plugin_scenarios:
          - scenarios/startx_openshift_logging.yml
          - scenarios/startx_openshift_monitoring.yml
          - scenarios/startx_openshift_machines.yml
          - scenarios/startx_openshift-apiserver.yml
          - scenarios/startx_openshift_etcd_pod.yml
          - scenarios/startx_openshift_podmonkey.yml
        - container_scenarios: 
          - - scenarios/startx_openshift_etcd_container.yml
        - time_scenarios: 
          - scenarios/startx_time_pod.yml
          - scenarios/startx_time_node.yml
        - node_scenarios: 
          - scenarios/startx_node_master.yml
          - scenarios/startx_node_infra.yml
          - scenarios/startx_node_app.yml
        - network_chaos:
          - scenarios/startx_node_app_network_chaos.yaml
          - scenarios/startx_node_infra_network_chaos.yaml
          - scenarios/startx_node_master_network_chaos.yaml
        - zone_outages:
          - scenarios/startx_zone_outage.yaml
        - cluster_shut_down_scenarios:
          - - scenarios/cluster_shut_down_scenario.yml
          # - scenarios/post_action_shut_down.py
        - application_outages:
          - scenarios/app_outage.yaml
        - pvc_scenarios:
          - scenarios/pvc_scenario.yaml
    cerberus:
        cerberus_enabled: True
        cerberus_url: {{ $root.Values.kraken.cerberusUrl | quote }}
        check_applicaton_routes: True
    performance_monitoring:
        deploy_dashboards: False
        repo: "https://github.com/cloud-bulldozer/performance-dashboards.git"
        kube_burner_binary_url: "https://github.com/cloud-bulldozer/kube-burner/releases/download/v1.7.2/kube-burner-V1.7.2-linux-x86_64.tar.gz"
        capture_metrics: True
        config_path: config/kube_burner.yaml
        metrics_profile_path: config/metrics-aggregated.yaml
        prometheus_url:
        prometheus_bearer_token:
        uuid:
        enable_alerts: False
        alert_profile: config/alerts
    tunings:
        wait_duration: 420
        iterations: 1
        daemon_mode: False
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kraken-config-common
  namespace: "{{- $namespace -}}"
  labels:
    app.kubernetes.io/name: "kraken-config-common"
    {{- include "chaos-kraken.labels" $root | nindent 4 }}
  annotations:
    {{- include "chaos-kraken.annotations" $root | nindent 4 }}
data:
  startx_alerts.yaml: |-
    - expr: avg_over_time(histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket[2m]))[5m:]) > 0.01
      description: 5 minutes avg. etcd fsync latency on {{`{{`}}$labels.pod{{`}}`}} higher than 10ms {{`{{`}}$value{{`}}`}}
      severity: error
    - expr: avg_over_time(histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket[5m]))[5m:]) > 0.1
      description: 5 minutes avg. etcd netowrk peer round trip on {{`{{`}}$labels.pod{{`}}`}} higher than 100ms {{`{{`}}$value{{`}}`}}
      severity: info
    - expr: increase(etcd_server_leader_changes_seen_total[2m]) > 0
      description: etcd leader changes observed
      severity: critical
  kube_burner.yaml: |-
    ---
    global:
      writeToFile: true
      metricsDirectory: collected-metrics
      measurements:
        - name: podLatency
          esIndex: kraken
      indexerConfig:
        enabled: true
        esServers: [http://elasticsearch.openshift-logging.svc.cluster.local:9200]
        insecureSkipVerify: true
        defaultIndex: kraken
        type: elastic
  metrics-aggregated.yaml: |-
    metrics:
    # API server
      - query: histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket{apiserver="kube-apiserver", verb!~"WATCH", subresource!="log"}[2m])) by (verb,resource,subresource,instance,le)) > 0
        metricName: API99thLatency
      - query: sum(irate(apiserver_request_total{apiserver="kube-apiserver",verb!="WATCH",subresource!="log"}[2m])) by (verb,instance,resource,code) > 0
        metricName: APIRequestRate
      - query: sum(apiserver_current_inflight_requests{}) by (request_kind) > 0
        metricName: APIInflightRequests
    # Container & pod metrics
      - query: (sum(container_memory_rss{name!="",container!="POD",namespace=~"openshift-(etcd|oauth-apiserver|.*apiserver|ovn-kubernetes|sdn|ingress|authentication|.*controller-manager|.*scheduler)"}) by (container, pod, namespace, node) and on (node) kube_node_role{role="master"}) > 0
        metricName: containerMemory-Masters
      - query: (sum(irate(container_cpu_usage_seconds_total{name!="",container!="POD",namespace=~"openshift-(etcd|oauth-apiserver|sdn|ovn-kubernetes|.*apiserver|authentication|.*controller-manager|.*scheduler)"}[2m]) * 100) by (container, pod, namespace, node) and on (node) kube_node_role{role="master"}) > 0
        metricName: containerCPU-Masters
      - query: (sum(irate(container_cpu_usage_seconds_total{pod!="",container="prometheus",namespace="openshift-monitoring"}[2m]) * 100) by (container, pod, namespace, node) and on (node) kube_node_role{role="infra"}) > 0
        metricName: containerCPU-Prometheus
      - query: (avg(irate(container_cpu_usage_seconds_total{name!="",container!="POD",namespace=~"openshift-(sdn|ovn-kubernetes|ingress)"}[2m]) * 100 and on (node) kube_node_role{role="worker"}) by (namespace, container)) > 0
        metricName: containerCPU-AggregatedWorkers
      - query: (avg(irate(container_cpu_usage_seconds_total{name!="",container!="POD",namespace=~"openshift-(sdn|ovn-kubernetes|ingress|monitoring|image-registry|logging)"}[2m]) * 100 and on (node) kube_node_role{role="infra"}) by (namespace, container)) > 0
        metricName: containerCPU-AggregatedInfra
      - query: (sum(container_memory_rss{pod!="",namespace="openshift-monitoring",name!="",container="prometheus"}) by (container, pod, namespace, node) and on (node) kube_node_role{role="infra"}) > 0
        metricName: containerMemory-Prometheus
      - query: avg(container_memory_rss{name!="",container!="POD",namespace=~"openshift-(sdn|ovn-kubernetes|ingress)"} and on (node) kube_node_role{role="worker"}) by (container, namespace)
        metricName: containerMemory-AggregatedWorkers
      - query: avg(container_memory_rss{name!="",container!="POD",namespace=~"openshift-(sdn|ovn-kubernetes|ingress|monitoring|image-registry|logging)"} and on (node) kube_node_role{role="infra"}) by (container, namespace)
        metricName: containerMemory-AggregatedInfra
    # Node metrics
      - query: (sum(irate(node_cpu_seconds_total[2m])) by (mode,instance) and on (instance) label_replace(kube_node_role{role="master"}, "instance", "$1", "node", "(.+)")) > 0
        metricName: nodeCPU-Masters
      - query: (avg((sum(irate(node_cpu_seconds_total[2m])) by (mode,instance) and on (instance) label_replace(kube_node_role{role="worker"}, "instance", "$1", "node", "(.+)"))) by (mode)) > 0
        metricName: nodeCPU-AggregatedWorkers
      - query: (avg((sum(irate(node_cpu_seconds_total[2m])) by (mode,instance) and on (instance) label_replace(kube_node_role{role="infra"}, "instance", "$1", "node", "(.+)"))) by (mode)) > 0
        metricName: nodeCPU-AggregatedInfra
      - query: avg(node_memory_MemAvailable_bytes) by (instance) and on (instance) label_replace(kube_node_role{role="master"}, "instance", "$1", "node", "(.+)")
        metricName: nodeMemoryAvailable-Masters
      - query: avg(node_memory_MemAvailable_bytes and on (instance) label_replace(kube_node_role{role="worker"}, "instance", "$1", "node", "(.+)"))
        metricName: nodeMemoryAvailable-AggregatedWorkers
      - query: avg(node_memory_MemAvailable_bytes and on (instance) label_replace(kube_node_role{role="infra"}, "instance", "$1", "node", "(.+)"))
        metricName: nodeMemoryAvailable-AggregatedInfra
      - query: avg(node_memory_Active_bytes) by (instance) and on (instance) label_replace(kube_node_role{role="master"}, "instance", "$1", "node", "(.+)")
        metricName: nodeMemoryActive-Masters
      - query: avg(node_memory_Active_bytes and on (instance) label_replace(kube_node_role{role="worker"}, "instance", "$1", "node", "(.+)"))
        metricName: nodeMemoryActive-AggregatedWorkers
      - query: avg(avg(node_memory_Active_bytes) by (instance) and on (instance) label_replace(kube_node_role{role="infra"}, "instance", "$1", "node", "(.+)"))
        metricName: nodeMemoryActive-AggregatedInfra
      - query: avg(node_memory_Cached_bytes) by (instance) + avg(node_memory_Buffers_bytes) by (instance) and on (instance) label_replace(kube_node_role{role="master"}, "instance", "$1", "node", "(.+)")
        metricName: nodeMemoryCached+nodeMemoryBuffers-Masters
      - query: avg(node_memory_Cached_bytes + node_memory_Buffers_bytes and on (instance) label_replace(kube_node_role{role="worker"}, "instance", "$1", "node", "(.+)"))
        metricName: nodeMemoryCached+nodeMemoryBuffers-AggregatedWorkers
      - query: avg(node_memory_Cached_bytes + node_memory_Buffers_bytes and on (instance) label_replace(kube_node_role{role="infra"}, "instance", "$1", "node", "(.+)"))
        metricName: nodeMemoryCached+nodeMemoryBuffers-AggregatedInfra
      - query: irate(node_network_receive_bytes_total{device=~"^(ens|eth|bond|team).*"}[2m]) and on (instance) label_replace(kube_node_role{role="master"}, "instance", "$1", "node", "(.+)")
        metricName: rxNetworkBytes-Masters
      - query: avg(irate(node_network_receive_bytes_total{device=~"^(ens|eth|bond|team).*"}[2m]) and on (instance) label_replace(kube_node_role{role="worker"}, "instance", "$1", "node", "(.+)")) by (device)
        metricName: rxNetworkBytes-AggregatedWorkers
      - query: avg(irate(node_network_receive_bytes_total{device=~"^(ens|eth|bond|team).*"}[2m]) and on (instance) label_replace(kube_node_role{role="infra"}, "instance", "$1", "node", "(.+)")) by (device)
        metricName: rxNetworkBytes-AggregatedInfra
      - query: irate(node_network_transmit_bytes_total{device=~"^(ens|eth|bond|team).*"}[2m]) and on (instance) label_replace(kube_node_role{role="master"}, "instance", "$1", "node", "(.+)")
        metricName: txNetworkBytes-Masters
      - query: avg(irate(node_network_transmit_bytes_total{device=~"^(ens|eth|bond|team).*"}[2m]) and on (instance) label_replace(kube_node_role{role="worker"}, "instance", "$1", "node", "(.+)")) by (device)
        metricName: txNetworkBytes-AggregatedWorkers
      - query: avg(irate(node_network_transmit_bytes_total{device=~"^(ens|eth|bond|team).*"}[2m]) and on (instance) label_replace(kube_node_role{role="infra"}, "instance", "$1", "node", "(.+)")) by (device)
        metricName: txNetworkBytes-AggregatedInfra
      - query: rate(node_disk_written_bytes_total{device!~"^(dm|rb).*"}[2m]) and on (instance) label_replace(kube_node_role{role="master"}, "instance", "$1", "node", "(.+)")
        metricName: nodeDiskWrittenBytes-Masters
      - query: avg(rate(node_disk_written_bytes_total{device!~"^(dm|rb).*"}[2m]) and on (instance) label_replace(kube_node_role{role="worker"}, "instance", "$1", "node", "(.+)")) by (device)
        metricName: nodeDiskWrittenBytes-AggregatedWorkers
      - query: avg(rate(node_disk_written_bytes_total{device!~"^(dm|rb).*"}[2m]) and on (instance) label_replace(kube_node_role{role="infra"}, "instance", "$1", "node", "(.+)")) by (device)
        metricName: nodeDiskWrittenBytes-AggregatedInfra
      - query: rate(node_disk_read_bytes_total{device!~"^(dm|rb).*"}[2m]) and on (instance) label_replace(kube_node_role{role="master"}, "instance", "$1", "node", "(.+)")
        metricName: nodeDiskReadBytes-Masters
      - query: avg(rate(node_disk_read_bytes_total{device!~"^(dm|rb).*"}[2m]) and on (instance) label_replace(kube_node_role{role="worker"}, "instance", "$1", "node", "(.+)")) by (device)
        metricName: nodeDiskReadBytes-AggregatedWorkers
      - query: avg(rate(node_disk_read_bytes_total{device!~"^(dm|rb).*"}[2m]) and on (instance) label_replace(kube_node_role{role="infra"}, "instance", "$1", "node", "(.+)")) by (device)
        metricName: nodeDiskReadBytes-AggregatedInfra
    # Etcd metrics
      - query: sum(rate(etcd_server_leader_changes_seen_total[2m]))
        metricName: etcdLeaderChangesRate
      - query: etcd_server_is_leader > 0
        metricName: etcdServerIsLeader
      - query: histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket[2m]))
        metricName: 99thEtcdDiskBackendCommitDurationSeconds
      - query: histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket[2m]))
        metricName: 99thEtcdDiskWalFsyncDurationSeconds
      - query: histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket[5m]))
        metricName: 99thEtcdRoundTripTimeSeconds
      - query: etcd_mvcc_db_total_size_in_bytes
        metricName: etcdDBPhysicalSizeBytes
      - query: etcd_mvcc_db_total_size_in_use_in_bytes
        metricName: etcdDBLogicalSizeBytes
      - query: sum by (cluster_version)(etcd_cluster_version)
        metricName: etcdVersion
        instant: true
      - query: sum(rate(etcd_object_counts{}[5m])) by (resource) > 0
        metricName: etcdObjectCount
      - query: histogram_quantile(0.99,sum(rate(etcd_request_duration_seconds_bucket[2m])) by (le,operation,apiserver)) > 0
        metricName: P99APIEtcdRequestLatency
    # Cluster metrics
      - query: count(kube_namespace_created)
        metricName: namespaceCount
      - query: sum(kube_pod_status_phase{}) by (phase)
        metricName: podStatusCount
      - query: count(kube_secret_info{})
        metricName: secretCount
      - query: count(kube_deployment_labels{})
        metricName: deploymentCount
      - query: count(kube_configmap_info{})
        metricName: configmapCount
      - query: count(kube_service_info{})
        metricName: serviceCount
      - query: kube_node_role
        metricName: nodeRoles
        instant: true
      - query: sum(kube_node_status_condition{status="true"}) by (condition)
        metricName: nodeStatus
      - query: (sum(rate(container_fs_writes_bytes_total{container!="",device!~".+dm.+"}[5m])) by (device, container, node) and on (node) kube_node_role{role="master"}) > 0
        metricName: containerDiskUsage
      - query: cluster_version{type="completed"}
        metricName: clusterVersion
        instant: true
    # Golang metrics
      - query: go_memstats_heap_alloc_bytes{job=~"apiserver|api|etcd"}
        metricName: goHeapAllocBytes
      - query: go_memstats_heap_inuse_bytes{job=~"apiserver|api|etcd"}
        metricName: goHeapInuseBytes
      - query: go_gc_duration_seconds{job=~"apiserver|api|etcd",quantile="1"}
        metricName: goGCDurationSeconds
{{- end -}}
