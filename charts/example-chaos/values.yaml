

# # Use it to Overwrite the helm-chart name
#nameOverride: example-chaos
# # Use it to Overwrite the helm-chart version
#versionOverride: 0.0.1

# Application chaos context
context:  &context
# Name of the global scope for this application (organisational tenant)
 scope: myscope
# Name of the cluster running this application (plateform tenant)
 cluster: localhost
# Name of the environement for this application (ex: dev, factory, preprod or prod)
 environment: myenv
# Component name of this application (logical tenant)
 component: helm
# Application name (functionnal tenant, default use Chart name)
 app: example-chaos
# Version name of this application (default use Chart appVersion)
 version: "0.0.1"

# Configuration of the project (see https://helm-repository.readthedocs.io/en/latest/charts/project)
project: 
  enabled: false
  context: 
    <<: *context
  project: 
    enabled: true
    type: project
    name: example-chaos
    display_name: Chaos - Main
    description: Chaos test suite configured by STARTX

# Configuration of the cerberus project (see https://helm-repository.readthedocs.io/en/latest/charts/project)
cerberus_project: 
  enabled: false
  context: 
    <<: *context
  project: 
    enabled: true
    type: project
    name: example-chaos-cerberus
    display_name: Chaos - Cerberus
    description: Cerberus watchdog in STARTX Chaos test suite

# Configuration of the cerberus deployment
cerberus: 
  # Depoy the cerberus watchdog
  enabled: false
  # Enable exposition for this application (route based)
  expose: true
  # Url list of endpoint to watch as part of the global healthcheck (double array)
  watch_url_routes: []
  # Kubeconfig of the supervised tested cluster (mandatory)
  kubeconfig:
    # Connection mode to use for the cluster (could be token or file)
    mode: token
    # If mode is token, this section must be filled
    token: 
      # The server URL to the target cluster API
      server: https://localhost:6443
      # The token to use to get access. This token must have full 
      # cluster admin access to perform some chaos scenarios
      token: sha256~XXXXXXXXXX_PUT_YOUR_TOKEN_HERE_XXXXXXXXXXXX
    # # If mode is token, this property must be set with a full kubeconfig content
    # file: |
    #   kind: Config
    #   apiVersion: v1
    #   current-context: default
    #   clusters:
    #   - name: default
    #     cluster:
    #       certificate-authority-data: XXXXXXX
    #       server: https://localhost:6443
    #   users:
    #   - name: default
    #     user:
    #       client-certificate-data: XXXXXXX
    #       client-key-data: XXXXXXX
    #   contexts:
    #   - name: default
    #     context:
    #       cluster: default
    #       namespace: default
    #       user: default
    #   preferences: {}

# Configuration of the kraken project (see https://helm-repository.readthedocs.io/en/latest/charts/project)
kraken_project: 
  enabled: false
  context: 
    <<: *context
  project: 
    enabled: true
    type: project
    name: example-chaos-kraken
    display_name: Chaos - Kraken
    description: Kraken chaos orchestrator in STARTX Chaos test suite

# Configuration of the kraken deployment
kraken: 
  # Unleash the kraken ;)
  enabled: false
  # Enable exposition for this application (route based)
  expose: true
  # Kraken mode (could be job or pipeline)
  mode: pipeline
  # configuration of the job mode
  job: 
    # Job name prefix
    prefix: kraken-test
  # configuration of the pipeline mode
  pipeline: 
    # pipelinerun name prefix
    prefix: kraken-test
  # AWS configuration section
  aws: 
    # enable the aws integration
    enabled: false
    # credentials (mandatory if enabled)
    credentials: 
      # used region
      region: eu-west-3
      # AWS access key ID
      key_id: AKIAXXXXXXXXXXXXXXXX
      # AWS secret access key
      secret: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
  # Kubeconfig of the chaos tested cluster (mandatory)
  kubeconfig:
    # Connection mode to use for the cluster (could be token or file)
    mode: token
    # If mode is token, this section must be filled
    token: 
      # The server URL to the target cluster API
      server: https://localhost:6443
      # The token to use to get access. This token must have full 
      # cluster admin access to perform some chaos scenarios
      token: sha256~XXXXXXXXXX_PUT_YOUR_TOKEN_HERE_XXXXXXXXXXXX
    # # If mode is token, this property must be set with a full kubeconfig content
    # file: |
    #   kind: Config
    #   apiVersion: v1
    #   current-context: default
    #   clusters:
    #   - name: default
    #     cluster:
    #       certificate-authority-data: XXXXXXX
    #       server: https://localhost:6443
    #   users:
    #   - name: default
    #     user:
    #       client-certificate-data: XXXXXXX
    #       client-key-data: XXXXXXX
    #   contexts:
    #   - name: default
    #     context:
    #       cluster: default
    #       namespace: default
    #       user: default
    #   preferences: {}

# Configuration of the mesh project (see https://helm-repository.readthedocs.io/en/latest/charts/project)
mesh_project: 
  enabled: false
  context: 
    <<: *context
  project: 
    enabled: true
    type: project
    name: example-chaos-mesh
    display_name: Chaos - Mesh
    description: Chaos-mesh chaos orchestrator in STARTX Chaos test suite

# Configuration of the mesh deployment (see https://charts.chaos-mesh.org)
mesh: 
  enabled: false
  expose: true
  chaosDaemon:
    runtime: crio
    socketPath: /var/run/crio/crio.sock
  nameOverride: ""
  fullnameOverride: ""
  customLabels: {}
  clusterScoped: true
  rbac:
    create: true
  timezone: "Europe/Paris"
  enableProfiling: true
  enableCtrlServer: true
  imagePullSecrets: []
  controllerManager: 
    hostNetwork: false
    serviceAccount: chaos-controller-manager
    replicaCount: 3
    env:
      WEBHOOK_PORT: 9443
      METRICS_PORT: 10080
    enableFilterNamespace: false
    service:
      type: ClusterIP
    resources: {}
    nodeSelector: {}
    tolerations: []
    affinity: {}
    podAnnotations: {}
    chaosdSecurityMode: true
  chaosDaemon:
    grpcPort: 31767
    httpPort: 31766
    env: {}
    hostNetwork: false
    mtls:
      enabled: false
    privileged: true
    podAnnotations: {}
    serviceAccount: chaos-daemon
    podSecurityPolicy: false
    runtime: crio
    socketPath: /var/run/crio/crio.sock
    resources: {}
    nodeSelector: {}
    tolerations: []
    affinity: {}
  dashboard:
    create: true
    rootUrl: https://ui-startx-chaos-mesh.apps.mycluster.com
    hostNetwork: false
    replicaCount: 1
    serviceAccount: chaos-dashboard
    imagePullPolicy: IfNotPresent
    securityMode: true
    nodeSelector: {}
    tolerations: []
    affinity: {}
    podAnnotations: {}
    service:
      annotations: {}
      type: ClusterIP
    resources:
      limits: {}
      requests:
        cpu: 25m
        memory: 256Mi
    persistentVolume:
      enabled: false
    env:
      LISTEN_HOST: 0.0.0.0
      LISTEN_PORT: 2333
      METRIC_HOST: 0.0.0.0
      METRIC_PORT: 2334
      DATABASE_DRIVER: sqlite3
      DATABASE_DATASOURCE: /data/core.sqlite
      CLEAN_SYNC_PERIOD: 12h
      TTL_EVENT: 168h
      TTL_EXPERIMENT: 336h
      TTL_SCHEDULE: 336h
      TTL_WORKFLOW: 336h
    ingress:
      enabled: false
  dnsServer:
    create: true
    serviceAccount: chaos-dns-server
    nodeSelector: {}
    tolerations: []
    podAnnotations: {}
    name: chaos-mesh-dns-server
    grpcPort: 9288
    resources:
      limits: {}
      requests:
        cpu: 100m
        memory: 70Mi
    env:
      LISTEN_HOST: "0.0.0.0"
      LISTEN_PORT: 53
  prometheus:
    create: true
    serviceAccount: prometheus
    nodeSelector: {}
    tolerations: []
    affinity: {}
    podAnnotations: {}
    resources:
      limits: {}
      requests:
        cpu: 250m
        memory: 512Mi
    service:
      type: ClusterIP
    volume:
      storage: 2Gi
      storageClassName: gp3-csi
  webhook:
    certManager:
      enabled: false
    timeoutSeconds: 5
    FailurePolicy: Fail
    CRDS:
      - podchaos
      - iochaos
      - timechaos
      - networkchaos
      - kernelchaos
      - stresschaos
      - awschaos
      - azurechaos
      - gcpchaos
      - dnschaos
      - jvmchaos
      - schedule
      - workflow
      - httpchaos
      - physicalmachinechaos
      - physicalmachine
      - statuscheck
  bpfki:
    create: true
    grpcPort: 50051
    resources: {}
  chaosDlv:
    enable: true

# Configuration of the kubemonkey project (see https://helm-repository.readthedocs.io/en/latest/charts/project)
monkey_project: 
  enabled: false
  context: 
    <<: *context
  project: 
    enabled: true
    type: project
    name: example-chaos-kubemonkey
    display_name: Chaos - Monkey
    description: Kubernetes chaos monkey in STARTX Chaos test suite

# Configuration of the kubemonkey deployment (see https://github.com/asobti/kube-monkey)
monkey:
  enabled: false
  nameOverride: monkey
  fullnameOverride: example-monkey
  replicaCount: 1
  rbac:
    enabled: true
  image:
    repository: ayushsobti/kube-monkey
    tag: v0.4.1
    pullPolicy: IfNotPresent
  config:
    dryRun: false
    runHour: 8
    startHour: 10
    endHour: 16
    blacklistedNamespaces: [ "kube-system", "openshift-*" ]
    whitelistedNamespaces: [ "default", "demo-*", "example-*" ]
    timeZone: Europe/Paris
  args:
    logLevel: 5
    logDir: /var/log/kube-monkey
  resources: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

# Configuration of the litmuschaos project (see https://helm-repository.readthedocs.io/en/latest/charts/project)
litmus_project: 
  enabled: false
  context: 
    <<: *context
  project: 
    enabled: true
    type: project
    name: example-chaos-litmus
    display_name: Chaos - Litmus
    description: Litmus chaos orchestrator in STARTX Chaos test suite

# Configuration of the litmus deployment (see https://litmuschaos.github.io/litmus-helm)
litmus:
  enabled: false
  nameOverride: litmus
  portalScope: cluster
  customLabels: {}
  adminConfig:
    DBUSER: "admin"
    DBPASSWORD: "1234"
    JWTSecret: "litmus-portal@123"
    VERSION: "2.9.0"
    SKIP_SSL_VERIFY: "false"
    # -- leave empty if uses Mongo DB deployed by this chart
    DB_SERVER: ""
    DB_PORT: "27017"
    ADMIN_USERNAME: "admin"
    ADMIN_PASSWORD: "litmus"
  image:
    imageRegistryName: litmuschaos
    imagePullSecrets: []
  ingress:
    enabled: false
    name: litmus-ingress
  upgradeAgent:
    controlPlane:
      image:
        repository: upgrade-agent-cp
        tag: "2.9.0"
        pullPolicy: "Always"
      restartPolicy: OnFailure
    nodeSelector: {}
    tolerations: []
    affinity: {}
  portal:
    frontend:
      replicas: 1
      updateStrategy: {}
      automountServiceAccountToken: false
      securityContext:
        runAsUser: 2000
        allowPrivilegeEscalation: false
        runAsNonRoot: true
      image:
        repository: litmusportal-frontend
        tag: 2.9.0
        pullPolicy: "Always"
      containerPort: 8080
      customLabels: {}
      resources:
        requests:
          memory: "150Mi"
          cpu: "125m"
          ephemeral-storage: "500Mi"
        limits:
          memory: "512Mi"
          cpu: "550m"
          ephemeral-storage: "1Gi"
      livenessProbe:
        failureThreshold: 5
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      readinessProbe:
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      service:
        annotations: {}
        type: NodePort
        port: 9091
        targetPort: 8080
      virtualService:
        enabled: false
        hosts: []
        gateways: []
      nodeSelector: {}
      tolerations: []
      affinity: {}
    server:
      replicas: 1
      updateStrategy: {}
      serviceAccountName: litmus-server-account
      customLabels: {}
      waitForMongodb:
        image:
          repository: curl
          tag: 2.9.0
          pullPolicy: "Always"
        resources:
          requests:
            memory: "150Mi"
            cpu: "25m"
            ephemeral-storage: "500Mi"
          limits:
            memory: "512Mi"
            cpu: "250m"
            ephemeral-storage: "1Gi"
      graphqlServer:
        volumes:
          - name: gitops-storage
            emptyDir: {}
          - name: hub-storage
            emptyDir: {}
        volumeMounts:
          - mountPath: /tmp/gitops
            name: gitops-storage
          - mountPath: /tmp/version
            name: hub-storage
        securityContext:
          runAsUser: 2000
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          readOnlyRootFilesystem: true
        image:
          repository: litmusportal-server
          tag: 2.9.0
          pullPolicy: "Always"
        ports:
          - name: gql-server
            containerPort: 8080
          - name: gql-rpc-server
            containerPort: 8000
        imageEnv:
          SUBSCRIBER_IMAGE: "litmusportal-subscriber:2.9.0"
          EVENT_TRACKER_IMAGE: "litmusportal-event-tracker:2.9.0"
          ARGO_WORKFLOW_CONTROLLER_IMAGE: "workflow-controller:v3.2.9"
          ARGO_WORKFLOW_EXECUTOR_IMAGE: "argoexec:v3.2.9"
          LITMUS_CHAOS_OPERATOR_IMAGE: "chaos-operator:2.8.0"
          LITMUS_CHAOS_RUNNER_IMAGE: "chaos-runner:2.8.0"
          LITMUS_CHAOS_EXPORTER_IMAGE: "chaos-exporter:2.8.0"
        genericEnv:
          TLS_SECRET_NAME: ""
          TLS_CERT_64: ""
          SELF_AGENT: "true"
          SELF_AGENT_NODE_SELECTOR: ""
          SELF_AGENT_TOLERATIONS: ""
          CONTAINER_RUNTIME_EXECUTOR: "k8sapi"
          HUB_BRANCH_NAME: "v2.8.x"
          AGENT_DEPLOYMENTS: '["app=chaos-exporter", "name=chaos-operator", "app=event-tracker", "app=workflow-controller"]'
          LITMUS_AUTH_GRPC_PORT: ":3030"
          LITMUS_CORE_VERSION: "2.8.0"
        resources:
          requests:
            memory: "250Mi"
            cpu: "225m"
            ephemeral-storage: "500Mi"
          limits:
            memory: "712Mi"
            cpu: "550m"
            ephemeral-storage: "1Gi"
        livenessProbe:
          failureThreshold: 5
          initialDelaySeconds: 30
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        readinessProbe:
          initialDelaySeconds: 5
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
      authServer:
        securityContext:
          runAsUser: 2000
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          readOnlyRootFilesystem: true
        automountServiceAccountToken: false
        image:
          repository: litmusportal-auth-server
          tag: 2.9.0
          pullPolicy: "Always"
        ports:
          - name: auth-server
            containerPort: 3030
          - name: auth-rpc-server
            containerPort: 3000
        env:
          LITMUS_GQL_GRPC_PORT: ":8000"
        resources:
          requests:
            memory: "250Mi"
            cpu: "225m"
            ephemeral-storage: "500Mi"
          limits:
            memory: "712Mi"
            cpu: "550m"
            ephemeral-storage: "1Gi"
      service:
        annotations: {}
        type: NodePort
        graphqlServer:
          port: 9002
          targetPort: 8080
        graphqlRpcServer:
          port: 8000
          targetPort: 8000
        authServer:
          port: 9003
          targetPort: 3000
        authRpcServer:
          port: 3030
          targetPort: 3030
      nodeSelector: {}
      tolerations: []
      affinity: {}
  mongo:
    replicas: 1
    customLabels: {}
    automountServiceAccountToken: false
    securityContext:
      # runAsUser: 2000
      allowPrivilegeEscalation: false
      # runAsNonRoot: true
    image:
      repository: mongo
      tag: 4.2.8
      pullPolicy: "Always"
    containerPort: 27017
    resources:
      requests:
        memory: "300Mi"
        cpu: "225m"
        ephemeral-storage: "1Gi"
      limits:
        memory: "712Mi"
        cpu: "550m"
        ephemeral-storage: "3Gi"
    livenessProbe:
      failureThreshold: 5
      initialDelaySeconds: 30
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 5
    readinessProbe:
      initialDelaySeconds: 5
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    persistence:
      size: 20Gi
      accessMode: ReadWriteOnce
      # storageClass: ""
    service:
      type: ClusterIP
      port: 27017
      targetPort: 27017
    # serviceAccountName: "mongo-service-account"
    nodeSelector: {}
    tolerations: []
    affinity: {}
  openshift:
    route:
      enabled: true
      name: litmus-portal
      annotations: {}
      host: ""
